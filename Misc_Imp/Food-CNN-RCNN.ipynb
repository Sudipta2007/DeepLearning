{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe Classification using simple CNN and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Recipies for Training:  39774\n",
      "No. of Recipies for Testing:  9944\n"
     ]
    }
   ],
   "source": [
    "recipeRaw = pd.read_json(\"/home/nbuser/challanges/08-recipe-ingredients/train.json\")\n",
    "recipeRaw[\"ingredientsFlat\"] = recipeRaw[\"ingredients\"].apply(lambda x: ' '.join(x))\n",
    "traindocs = recipeRaw[\"ingredientsFlat\"].values\n",
    "\n",
    "recipeRawTest = pd.read_json(\"/home/nbuser/challanges/08-recipe-ingredients/test.json\")\n",
    "recipeRawTest[\"ingredientsFlat\"] = recipeRawTest[\"ingredients\"].apply(lambda x: ' '.join(x))\n",
    "testdocs = recipeRawTest[\"ingredientsFlat\"].values\n",
    "\n",
    "print(\"No. of Recipies for Training: \", len(traindocs))\n",
    "print(\"No. of Recipies for Testing: \", len(testdocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindocs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average recipe length 65\n",
      "average recipe length 1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"/home/nbuser/challanges/08-recipe-ingredients/train.json\")['ingredients']\n",
    "print('average recipe length',max( map(len, df) ))\n",
    "print('average recipe length',min( map(len, df) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "italian         7838\n",
       "mexican         6438\n",
       "southern_us     4320\n",
       "indian          3003\n",
       "chinese         2673\n",
       "french          2646\n",
       "cajun_creole    1546\n",
       "thai            1539\n",
       "japanese        1423\n",
       "greek           1175\n",
       "spanish          989\n",
       "korean           830\n",
       "vietnamese       825\n",
       "moroccan         821\n",
       "british          804\n",
       "filipino         755\n",
       "irish            667\n",
       "jamaican         526\n",
       "russian          489\n",
       "brazilian        467\n",
       "Name: cuisine, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipeRaw[\"cuisine\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "leb = preprocessing.LabelEncoder()\n",
    "leb.fit(recipeRaw[\"cuisine\"].values)\n",
    "\n",
    "labels_enc = leb.transform(recipeRaw[\"cuisine\"].values)\n",
    "labels = to_categorical(labels_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the text (ingredients) of the recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Unique Ingredints: 3065\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(traindocs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "encoded_train_docs = t.texts_to_sequences(traindocs)\n",
    "encoded_test_docs = t.texts_to_sequences(testdocs)\n",
    "print(\"Total number of Unique Ingredints:\",vocab_size)\n",
    "\n",
    "max_length = 65\n",
    "padded_train_docs = pad_sequences(encoded_train_docs, maxlen=max_length, padding='post')\n",
    "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_train_docs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Glove word-embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/home/nbuser/NLP/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.DataFrame.from_dict(t.word_index,orient=\"index\")\n",
    "vocab.drop([0],axis=1).reset_index().rename(columns={\"index\":\"word\"}).to_csv(\"vocab.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Glove Word-embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Simple CNN-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 65, 100)           306500    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 65, 100)           30100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 264)               845064    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               33920     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 1,218,164\n",
      "Trainable params: 911,664\n",
      "Non-trainable params: 306,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "acc: 64.86%\n",
      "acc: 63.80%\n",
      "acc: 65.63%\n",
      "acc: 64.12%\n",
      "acc: 65.70%\n",
      "64.82% (+/- 0.77%)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "cvscores = []\n",
    "for train, valid in kfold.split(padded_train_docs, labels):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=65, trainable=False))\n",
    "    model.add(Conv1D(filters=100, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(264, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    if cvscores == []:\n",
    "        print(model.summary())\n",
    "\n",
    "    model.fit(padded_train_docs[train], labels[train], epochs=10, verbose=0)\n",
    "    scores = model.evaluate(padded_train_docs[valid], labels[valid], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(padded_test_docs)\n",
    "recipeRawTest[\"cuisine\"] = [leb.classes_[np.argmax(prediction)] for prediction in predictions]\n",
    "recipeRawTest.head()\n",
    "recipeRawTest.drop([\"ingredients\",\"ingredientsFlat\"],axis=1).to_csv(\"predictRecipe-CNN.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 32)          1280      \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 20)                660       \n",
      "=================================================================\n",
      "Total params: 4,020\n",
      "Trainable params: 4,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "acc: 21.33%\n",
      "acc: 15.16%\n",
      "acc: 19.45%\n",
      "acc: 20.24%\n",
      "acc: 19.69%\n",
      "19.17% (+/- 2.11%)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "cvscores = []\n",
    "for train, valid in kfold.split(padded_train_docs, labels):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(40, 32))\n",
    "    model.add(SimpleRNN(32))\n",
    "    model.add(Dense(20, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    if cvscores == []:\n",
    "        print(model.summary())\n",
    "\n",
    "    model.fit(padded_train_docs[train], labels[train], epochs=5, verbose=0)\n",
    "    scores = model.evaluate(padded_train_docs[valid], labels[valid], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
